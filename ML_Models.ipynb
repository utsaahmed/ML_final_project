# importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from mlxtend.evaluate import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix

from xgboost import XGBClassifier

import tensorflow as tf
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization

import pickle


# Final Score Evaluation
def scores(y_train, y_train_pred):
    A = accuracy_score(y_train, y_train_pred)
    P = precision_score(y_train, y_train_pred, average = None)[0]
    R = recall_score(y_train, y_train_pred, average = None)[0]
    F = f1_score(y_train, y_train_pred, average = None)[0]
    return np.array([A, P, R, F])
    
    
    
 # load dataset
data = pd.read_csv('international.csv') 

# function for the summary of dataset
def inspect_data(data):
    return pd.DataFrame({"Data Type":data.dtypes,"No of Levels":data.apply(lambda x: x.nunique(),axis=0), "Levels":data.apply(lambda x: str(x.unique()),axis=0)})
    
inspect_data(data)


data.isnull().sum() # The column 'City' has many null values which will have to be preprocessed


# Drop repeated rows
data = data.drop_duplicates(keep='first')
data.shape


# Dropping the columns which are of no use to us in the current problem
data.drop(['Dates', 'Match_Type', 'Outcome', 'Margin'], axis=1, inplace= True)


# Replace null values with 'NA' for further processing
data['City'].fillna('NA', inplace=True)


#outcome variable team1_win as a probability of team1 winning the match
data.loc[data["Winner"]==data["Team1"],"Team1_Win"]=1
data.loc[data["Winner"]!=data["Team1"],"Team1_Win"]=0
data.loc[data["Winner"]=='draw',"Team1_Win"]=2

#outcome variable team1_toss_win as a value of team1 winning the toss
data.loc[data["Toss"]==data["Team1"],"Team1_Toss_Win"]=1
data.loc[data["Toss"]!=data["Team1"],"Team1_Toss_Win"]=0

#outcome variable team1_bat to depict if team1 bats first
data["Team1_Bat"]=0
data.loc[(((data["Team1_Toss_Win"]==1) & (data["Choice"]=="bat")) | ((data["Team1_Toss_Win"]==0) & (data["Choice"]=="field"))),"Team1_Bat"]=1

#encoding the numeric values
encoder= LabelEncoder()
data["Team1"]=encoder.fit_transform(data["Team1"])
mapping_team1 = dict(zip(encoder.classes_, range(len(encoder.classes_))))
data["Team2"]=encoder.fit_transform(data["Team2"])
mapping_team2 = dict(zip(encoder.classes_, range(len(encoder.classes_))))
data["City"]=encoder.fit_transform(data["City"])
mapping_city = dict(zip(encoder.classes_, range(len(encoder.classes_))))
data["Gender"]=encoder.fit_transform(data["Gender"])
mapping_gender = dict(zip(encoder.classes_, range(len(encoder.classes_))))

%store mapping_team1
%store mapping_team2
%store mapping_city
%store mapping_gender

#Checking for the distribution of the dataset
sns.countplot(x="Team1_Win",data=data)
print('Legend:')
print('0: Team1 Loses')
print('1: Team1 Wins')
print('2: Match Ends in a Draw')
plt.show()

# Dropping the now redundant columns
data.drop(['Toss','Choice','Winner'], axis=1, inplace= True)

#dataframe of related features
prediction_df=data[["Gender","Team1","Team2","City","Overs","Team1_Toss_Win","Team1_Bat","Team1_Win"]]

#finding the higly correlated features
correlated_features = set()
correlation_matrix = prediction_df.drop('Team1_Win', axis=1).corr()

print(correlation_matrix)
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            column = correlation_matrix.columns[i]
            correlated_features.add(column)
            
prediction_df.drop(columns=correlated_features, inplace=True)
prediction_df.head()

print('0: Gender, 1: Team1, 2: Team2, 3: City, 4: Overs, 5: Team1_Toss_Win, 6: Team1_Bat')
plt.matshow(correlation_matrix)
plt.show()

data = data.astype('category')  # All the features are categorical in nature

X = prediction_df.drop('Team1_Win', axis=1)
y = prediction_df['Team1_Win']
y = y.astype(int)

np.unique(y, return_counts=True) # Number of unique classes in target

#Splitting the data into training and testing data and scaling it
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)

# The 'Comparison' dataset will contain the metric scores for final comparison between models
Comparison = pd.DataFrame(columns=['Decision Tree', 'Random Forest', 'XGBoost', 'Neural Net'], index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])

#Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of logistic regression classifier on test set: {:.4f}'.format(logreg.score(X_test, y_test)))
Comparison['Logistic Regression'] = scores(y_test, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)

#SVM
svm=SVC()
svm.fit(X_train,y_train)
svm.score(X_test,y_test)
y_pred = svm.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of SVM classifier on test set: {:.4f}'.format(svm.score(X_test, y_test)))
Comparison['Support Vector Machine'] = scores(y_test, y_pred)


# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)


#Decision Tree Classifier
dtree=DecisionTreeClassifier()
dtree.fit(X_train,y_train)
dtree.score(X_test,y_test)
y_pred = dtree.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of decision tree classifier on test set: {:.4f}'.format(dtree.score(X_test, y_test)))
Comparison['Decision Tree'] = scores(y_test, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)

#Random Forest Classifier
randomForest= RandomForestClassifier(n_estimators=100)
randomForest.fit(X_train,y_train)
randomForest.score(X_test,y_test)
y_pred = randomForest.predict(X_test)
print("Confusion matrix\n",confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of random forest classifier on test set: {:.4f}'.format(randomForest.score(X_test, y_test)))
Comparison['Random Forest'] = scores(y_test, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)

model = Sequential()
model.add(Dense(12, input_dim=7, activation='relu', name='layer1'))
model.add(Dense(12, activation='relu', name='layer2'))
model.add(Dense(12, activation='relu', name='layer3'))
model.add(Dense(3, activation='softmax', name='layer4'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=150, batch_size=20)

model.evaluate(X_train, y_train, verbose=1)

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
print("Confusion matrix\n",confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of Neural Network on test set: ')
model.evaluate(X_test, y_test)
Comparison['Neural Net'] = scores(y_test, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)

#XGBoost
XGB = XGBClassifier(n_jobs=-1,)
%time XGB.fit(X = X_train, y=y_train)
XGB.score(X_test,y_test)
y_pred = XGB.predict(X_test)
print("Confusion matrix\n",confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('Accuracy of XGBoost classifier on test set: {:.4f}'.format(XGB.score(X_test, y_test)))
Comparison['XGBoost'] = scores(y_test, y_pred)

# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(XGB, open(filename, 'wb'))

# Confusion Matrix
cm = confusion_matrix(y_target=y_test, 
                      y_predicted=y_pred, 
                      binary=False,)

fig, ax = plot_confusion_matrix(conf_mat=cm, colorbar=True)

Comparison
